{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046fd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa as sm1\n",
    "from dateutil.parser import parse\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# To do remove the graph related libraries .\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#import seaborn as sns\n",
    "#import plotly.express as px\n",
    "#import chart_studio.plotly as ply\n",
    "#import cufflinks as cf\n",
    "#import cufflinks as cf\n",
    "#cf.go_offline()\n",
    "#cf.set_config_file(offline=False, world_readable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e3613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DB4\\AppData\\Local\\Temp\\ipykernel_53952\\981471624.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width: 100% !important}</style"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML ('<style>.container {width: 100% !important}</style'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9679969",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing CSV Dataset\n",
    "file = r\"C:\\Users\\plahare\\Downloads\\BeerWineLiquor.csv\"\n",
    "\n",
    "def file_read(data):\n",
    "    return pd.read_csv(data)# data= file_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee46538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>beer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/1992</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/1/1992</td>\n",
       "      <td>1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/1/1992</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/1/1992</td>\n",
       "      <td>1675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/1/1992</td>\n",
       "      <td>1822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>8/1/2018</td>\n",
       "      <td>4898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>9/1/2018</td>\n",
       "      <td>4598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>10/1/2018</td>\n",
       "      <td>4737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>11/1/2018</td>\n",
       "      <td>5130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>12/1/2018</td>\n",
       "      <td>6370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  beer\n",
       "0     1/1/1992  1509\n",
       "1     2/1/1992  1541\n",
       "2     3/1/1992  1597\n",
       "3     4/1/1992  1675\n",
       "4     5/1/1992  1822\n",
       "..         ...   ...\n",
       "319   8/1/2018  4898\n",
       "320   9/1/2018  4598\n",
       "321  10/1/2018  4737\n",
       "322  11/1/2018  5130\n",
       "323  12/1/2018  6370\n",
       "\n",
       "[324 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= file_read(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4e93c",
   "metadata": {},
   "source": [
    "# Setting Target col and Date col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8999d",
   "metadata": {},
   "source": [
    "### backend team, please create an object or drop _down where user can choose a target column since we are unable to call these functions in other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c8d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropdown for selecting target column for forecasting and date column \n",
    "def target_col(data,col_name):\n",
    "    target_column=data[col_name]\n",
    "    #df=pd.DataFrame(target_column)\n",
    "    #df.columns.names = ['Index']\n",
    "    return target_column\n",
    "    \n",
    "    \n",
    "def ts_col(data,col_name):\n",
    "    time_column=data[col_name]\n",
    "    #df=pd.DataFrame(time_column)\n",
    "    #df.columns.names = ['Index']\n",
    "    return time_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee546c6",
   "metadata": {},
   "source": [
    "# Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7603fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing date to datetime and set it as an index\n",
    "def set_index(data,date_column):\n",
    "    data.set_index(date_column,inplace=True)\n",
    "    data.index =[ x.strip() for x in data.index]\n",
    "    data.columns.names=[date_column]\n",
    "    data.index=pd.to_datetime(data.index)\n",
    "    \n",
    "    return data\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225d60c",
   "metadata": {},
   "source": [
    "# Shape of Data / Rows & Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa18f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying rows and columns #Return dictionary .eg:{\"rows\": 0, \"cols\": 0}\n",
    "def shape_df(data):\n",
    "    s=data.shape\n",
    "    s1=s[0]\n",
    "    s2=s[1]\n",
    "    df={'Name':['Row_count','Column_count'],'Count':[s1,s2]}\n",
    "    df1=pd.DataFrame(df)\n",
    "    df1.columns.names = ['Index']\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19026ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86335a6e",
   "metadata": {},
   "source": [
    "# Head & Tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad2a2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving choice to user to display head or tail  \n",
    "def display_head_tail1(data, choice='Head'):\n",
    "    if choice == 'Head':\n",
    "        df=data.head()\n",
    "        return df\n",
    "    elif choice=='Tail':\n",
    "        df=data.tail()\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        return {\"message\": \"Invalid choice\"} \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f39623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1998034f",
   "metadata": {},
   "source": [
    "# Describe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce492f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_description(dataframe):\n",
    "    a = dataframe.describe()\n",
    "    b = a.transpose()\n",
    "    d = pd.DataFrame({'col_names': pd.Series(dtype='str'),\n",
    "                   'col_data_type': pd.Series(dtype='str'),\n",
    "                   'col_null_val': pd.Series(dtype='int')})\n",
    "    l1 =[]\n",
    "    l2=[]\n",
    "    l3=[]\n",
    "    col = dataframe.columns\n",
    "    for i in col:\n",
    "        l1.append(i)\n",
    "        l2.append(df[i].dtypes)\n",
    "        l3.append(df[i].isnull().sum())\n",
    "    d['col_names']= l1\n",
    "    d['col_data_type']=l2\n",
    "    d['col_null_val']=l3\n",
    "    d = d.set_index(\"col_names\", drop=True)\n",
    "    d.index.name = None\n",
    "    desc = d.join(b)\n",
    "    desc.columns.names = ['Column_Name']\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be2d3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd351be1",
   "metadata": {},
   "source": [
    "### Resampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8be443d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exhibit frequency of data\n",
    "\n",
    "def exhhibitFreq(dataframe, col):\n",
    "    #dataframe = dataframe.reset_index()\n",
    "    dataframe = dataframe.sort_values(by=col)\n",
    "    dataframe[col] = pd.to_datetime(dataframe[col]) #to convert date column into datetime datatype\n",
    "    dataframe['Diff_Days'] = dataframe[col] - dataframe[col].shift(1)\n",
    "    dataframe['Diff_Days'] = pd.to_timedelta(dataframe['Diff_Days'])\n",
    "    ls = np.unique(dataframe['Diff_Days'])\n",
    "    dataframe.set_index(col)\n",
    "    ls = pd.to_timedelta(ls)\n",
    "    ls = ls.days\n",
    "    freq = []\n",
    "    for i in range(len(ls)):\n",
    "        freq.append(ls[i])\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237775de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling data\n",
    "\n",
    "def resamplingData1(dataframe, period, col):\n",
    "    #dataframe = dataframe.reset_index()\n",
    "    dataframe[col] = pd.to_datetime(dataframe[col])\n",
    "    dataframe = dataframe.resample(period, on=col).mean().reset_index(drop=False)\n",
    "    for i in dataframe.columns:\n",
    "        if dataframe.dtypes[i] == int or dataframe.dtypes[i] == float:\n",
    "            dataframe[i].interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "        else:\n",
    "            dataframe[i].interpolate(method='bfill', limit_direction=\"backward\", inplace=True)\n",
    "\n",
    "    return dataframe.set_index(col, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fc54a",
   "metadata": {},
   "source": [
    "# Null Value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0f9d6",
   "metadata": {},
   "source": [
    "## List of columns having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eff4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This functions creates a dictionary where columns are keys and values are percentage of null values present in that column \n",
    "def null_list(df):\n",
    "    \n",
    "    mydict={}#an empty dictionary for storing null value percentage\n",
    "    list1=[]\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().sum()>0: #this is to create a dictionary with columns which has null values.\n",
    "            mydict[i]=[(df.isnull().sum())*100 / len(df)][0][i]\n",
    "    for j,k in mydict.items():\n",
    "        list1.append(j)\n",
    "    \n",
    "    if len(list1)==0: \n",
    "        Message={\"Message\": \"This dataset doesn't have any null values , kindly proceed with the EDA \"} \n",
    "        \n",
    "        return Message\n",
    "    else:\n",
    "        return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73602b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a109cc52",
   "metadata": {},
   "source": [
    "## Graph to display percentage of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8290c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting the null values. this function plots graph of columns in the x-axis and its percentage of null values in the y-axis\n",
    "def graph(data):\n",
    "    mydict={}#an empty dictionary for storing the null values and its percentage\n",
    "    for i in data.columns:\n",
    "        mydict[i]=[(data.isnull().sum())*100 / len(data)][0][i]\n",
    "    null_percentage=mydict.values()\n",
    "    x=(np.array(data.columns)).tolist()\n",
    "    y=list(null_percentage)\n",
    "    my_dict={\"x_label\": 'Columns',\n",
    "             \"y_label\":'Pecentages',\n",
    "             \"title\": \"Percentage of null values present in each column\",\n",
    "             \"x_value\":x,\n",
    "             \"y_value\":y,\n",
    "             \"chart_type\":'bar'}\n",
    "    return my_dict  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac1c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e25157e",
   "metadata": {},
   "source": [
    "## Null Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf0644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217d7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18ffcbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################### Above function Edited by Vijay#############################################\n",
    "def impute(df, col_name, impute_method='interpolation'):\n",
    "    if df.dtypes[col_name] == str or df.dtypes[col_name] == object:\n",
    "        df[col_name].fillna(df[col_name].mode()[0], inplace=True)\n",
    "    else:\n",
    "        flag1 = (df[col_name].isnull() & df[col_name].shift(-1).isnull()).any()\n",
    "        flag2 = df[col_name].head(1).isnull().bool()\n",
    "        flag3 = df[col_name].tail(1).isnull().bool()\n",
    "        if flag1 or flag2 or flag3:\n",
    "            df[col_name].fillna(df[col_name].interpolate(method='linear', limit_direction=\"both\"), inplace=True)\n",
    "        elif impute_method == \"locf\" and (flag1 == False and flag2 == False and flag3 == False):\n",
    "            df[col_name].fillna(df[col_name].ffill(),inplace=True)\n",
    "        elif impute_method == \"nocb\" and (flag1 == False and flag2 == False and flag3 == False):\n",
    "            df[col_name].fillna(df[col_name].bfill(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "873de5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df, col_name):\n",
    "    df.dropna(subset=[col_name], axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e5d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df, col_name):\n",
    "    df.drop([col_name],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e8595",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c283b58",
   "metadata": {},
   "source": [
    "## Date vs target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e4dc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_col(df,target_col):\n",
    "    Dict={\"Interpretation\" : \"This graph represents visualization of dependent or target variable w.r.t Time.This depicts how the dependent variable varies with the time. X axis represents time and Y axis represents dependent variable. \"}\n",
    "   \n",
    "    my_dict={\"x_label\": 'Time',\n",
    "             \"y_label\":'Target column values',\n",
    "             \"title\": \"Target variable w.r.t. time\",\n",
    "             \"x_value\":df.index,\n",
    "             \"y_value\":df[target_col],\n",
    "             \"chart_type\":'lineplot'}\n",
    "    return my_dict, Dict\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ec5e585",
   "metadata": {},
   "source": [
    "## Resampled plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91043474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is from UI perspective\n",
    "def resample_plot(data,target_col,resample_alias=\"M\"):\n",
    "    A=pd.DataFrame(data[target_col].resample(resample_alias).max())\n",
    "    my_dict={\n",
    "        \"title\":\"Resampled Graph Of Target Variable\",\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Target Column',\n",
    "        \"x_values\":A.index,\n",
    "        \"y_values\":A.values,\n",
    "        \"Chart_type\":\"BarPlot\"}\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce986d",
   "metadata": {},
   "source": [
    "## Top n Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ced44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying top n values in dataframe\n",
    "def top_n_values(data,target_col,n=10):\n",
    "    #n = int(input(\"How many top values do you want to see?\\n\"))\n",
    "    #My_dict={\"Below are the top {0} values in the {1} column\": \".format(n, col_name)} \n",
    "    top_values=pd.DataFrame(data[target_col].sort_values(ascending = False).head(10))\n",
    "    top_values.columns.names = ['date']\n",
    "    return top_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c426653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n(data,target_col):\n",
    "    dict1={\"Interpretation\":\"\\n This graph represents visualization of Top values of dependent or target variable w.r.t Time. X axis represents time and Y axis represents top values of dependent variable. \"}\n",
    "    top_n_dataf =top_n_values(data,target_col,n=10)\n",
    "    my_dict={\n",
    "        \"title\":\"Visualization of Top N values Of Target Variable\",\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Target Column',\n",
    "        \"x_values\": data.index,\n",
    "        \"y_values\":top_n_dataf.values,\n",
    "        \"Chart_type\":\"BarPlot\"}\n",
    "    return my_dict,dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861a7c8",
   "metadata": {},
   "source": [
    "# Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43d2c7",
   "metadata": {},
   "source": [
    "## Seasonal Decompose during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea130726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal decomposition plot\n",
    "# seasonal decomposition plot\n",
    "def decomposition(target_col,choice='A'):\n",
    "    y = target_col.to_frame()\n",
    "    if choice == 'M':    \n",
    "    # Multiplicative Decomposition\n",
    "        result=seasonal_decompose(y, model='multiplicative')       \n",
    "        Date= y.index\n",
    "        \n",
    "        observed = result.observed\n",
    "        Trend_comp= result.trend\n",
    "        Seasonal_comp= result.seasonal\n",
    "        Residual_comp= result.resid\n",
    "        \n",
    "        mydict_Observed={\n",
    "        \"title\":\"Multiplicative Decompose\",\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Observed',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":observed.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "#         return mydict_Observed\n",
    "    \n",
    "    \n",
    "        mydict_Trend={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Trend',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":Trend_comp.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "#         return mydict_Trend\n",
    "      \n",
    "    \n",
    "        mydict_seasonal={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Seasonal',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":Seasonal_comp.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "#         return mydict_seasonal\n",
    "    \n",
    "        mydict_Resid={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Residual',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":Residual_comp.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "        \n",
    "        My_dict={\"Interpretation\" : \"Here X axis represents Time and Y axis represents Normal scaled data. Time series has 4 components Trend,seasonality,cyclical variation and irregular variation.\",\n",
    "\n",
    "                  \"Trend component\": \"This is useful in predicting future movements. Over a long period of time, the trend shows whether the data tends to increase or decrease\",\n",
    "\n",
    "                  \"Seasonal component\": \"The seasonal component of a time series is the variation in some variable due to some predetermined patterns in its behavior.\",\n",
    "\n",
    "                  \"Cyclical component\": \"The cyclical component in a time series is the part of the movement in the variable which can be explained by other cyclical movements in the economy.\",\n",
    "\n",
    "                  \"Irregular component\": \"this term gives information about non-seasonal patterns.\",\n",
    "\n",
    "                  \"Note\":\"Time series has two types of decomposition models Additive Model and Multiplicative model. The plot shows the decomposition of your time series data in its seasonal component, its trend component and the remainder. If you add or multiply the decomposition together you would get back the actual data. First block represents original series , second represents trend , third represents seasonality presents, fourth represents error component or residual.For additive if we add below three blocks we get original data series. Similarly for multiplicative we have to multiply the components.\"}\n",
    "\n",
    "        return mydict_Observed,mydict_Trend,mydict_seasonal,mydict_Resid,My_dict\n",
    "\n",
    "    elif choice == 'A':\n",
    "        \n",
    "    # Additive Decomposition\n",
    "        result=seasonal_decompose(y, model='additive')\n",
    "        \n",
    "        Date= y.index\n",
    "        observed = result.observed\n",
    "        Trend_comp= result.trend\n",
    "        Seasonal_comp= result.seasonal\n",
    "        Residual_comp= result.resid\n",
    "\n",
    "        mydict_Observed={\n",
    "        \"title\":\"Additive Decompose\",\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Observed',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":observed.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "#        return mydict_Observed\n",
    "    \n",
    "    \n",
    "        mydict_Trend={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Trend',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":Trend_comp.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "#        return mydict_Trend\n",
    "    \n",
    "        mydict_seasonal={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Seasonal',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":Seasonal_comp.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "#        return mydict_seasonal\n",
    "    \n",
    "        mydict_Resid={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Residual',\n",
    "        \"x_values\": Date,\n",
    "        \"y_values\":Residual_comp.values,\n",
    "        \"Chart_type\":\"LineChart\"}\n",
    "        \n",
    "\n",
    "        My_dict={\"Interpretation\" : \"Here X axis represents Time and Y axis represents Normal scaled data. Time series has 4 components Trend,seasonality,cyclical variation and irregular variation.\",\n",
    "\n",
    "                 \"Trend component\": \"This is useful in predicting future movements. Over a long period of time, the trend shows whether the data tends to increase or decrease\",\n",
    "\n",
    "                \"Seasonal component\": \"The seasonal component of a time series is the variation in some variable due to some predetermined patterns in its behavior.\",\n",
    "               \n",
    "                 \"Cyclical component\": \"The cyclical component in a time series is the part of the movement in the variable which can be explained by other cyclical movements in the economy.\",\n",
    "                \n",
    "                 \"irregular component\": \"this term gives information about non-seasonal patterns.\",\n",
    "\n",
    "                 \"Note\":\"Time series has two types of decomposition models Additive Model and Multiplicative model. The plot shows the decomposition of your time series data in its seasonal component, its trend component and the remainder. If you add or multiply the decomposition together you would get back the actual data. First block represents original series , second represents trend , third represents seasonality presents, fourth represents error component or residual.For additive if we add below three blocks we get original data series. Similarly for multiplicative we have to multiply the components.\"}\n",
    "\n",
    "        return mydict_Observed,mydict_Trend,mydict_seasonal,mydict_Resid,My_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b486b",
   "metadata": {},
   "source": [
    "## Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755af122",
   "metadata": {},
   "source": [
    "## Stationarity Check Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "923f7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for checking stationarity\n",
    "def stationarity_check_plot(timeseries,target_col):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "    \n",
    "    df_x= df.index\n",
    "    df_y= df[target_col]\n",
    "\n",
    "    x_rollmean=rolmean.index\n",
    "    y_rollmean=rolmean[target_col]\n",
    "    \n",
    "    x_rolstd=rolstd.index\n",
    "    y_rolstd=rolstd[target_col]\n",
    "    \n",
    "    \n",
    "    mydict_df={ \n",
    "        \"title\":\"Stationarity check Plot\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Target',\n",
    "        \"legends\" :['Original'],\n",
    "        \"x_values\":df_x,\n",
    "        \"y_values\":df_y,\n",
    "        \"Chart_type\":\"StackedLineChart\"\n",
    "    }\n",
    "   \n",
    "    mydict_rollmean={ \n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'RollingMean',\n",
    "        \"legends\":['RollingMean'],\n",
    "        \"x_values\": x_rollmean,\n",
    "        \"y_values\":y_rollmean,\n",
    "        \"Chart_type\":\"StackedLineChart\"}\n",
    "\n",
    "    mydict_rollstd={\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'RollingMean',\n",
    "        \"legends\":['RollingMean'],\n",
    "        \"x_values\": x_rollmean,\n",
    "        \"y_values\":y_rollmean,\n",
    "        \"Chart_type\":\"StackedLineChart\"}\n",
    "    \n",
    "    My_Dict={\"Interpretation\": \" \",\n",
    "             \n",
    "             \"Stationarity\": \" Stationarity means that the statistical properties of a process generating a time series do not change over time. That is Mean and Standard deviation is approximately constant over time.\\n\\nStationarity Graph represents stationarity of the series w.r.t. Time. X axis depicts time and Y axis depicts Dependent variable . Blue line represents the original Time series data , Red line represents Mean of the series data and Black line represents standard deviation of the series. \"}\n",
    "    \n",
    "    return mydict_df,mydict_rollmean,mydict_rollstd,My_Dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffcc33",
   "metadata": {},
   "source": [
    "### Stationarity test- adf , kpss , and conversion from stationary to non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_test3(data,series):\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data  \n",
    "    labels = ['ADF test statistic','p-value','# lags used','number_of_observations_used']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "    if result[1] < 0.05:        \n",
    "        out1=\"stationary\"\n",
    "    else:\n",
    "        out1=\"non_stationary\"\n",
    "        \n",
    "    statistic, p_value, n_lags, critical_values = kpss(series)\n",
    "    dict_kpss={\"KPSS test statistic\":statistic,\"p_value\":p_value,\"n_lags\":n_lags,\"critical_values\":critical_values}\n",
    "    #kpss_result=pd.DataFrame(dict_kpss.items(), columns=['KPSS test statistic', 'p_value','n_lags','critical_values'])\n",
    "    # Format Output\n",
    "    \n",
    "    if p_value<0.05:\n",
    "        out2=\"stationary\"\n",
    "    else:\n",
    "        out2=\"non_stationary\"\n",
    "    if out1==\"stationary\" and out2==\"stationary\":\n",
    "        dict_1={\"Message\":\"Since both ADF and KPSS test results indicates stationarity,the data is stationary. Kindly proceed further \"}\n",
    "        out_dict=out.to_dict()\n",
    "        out_dict[\"Message1\"]=\"Strong evidence against the null hypothesis\"\n",
    "        out_dict[\"Message2\"]=\"Data has no unit root and is stationary\"\n",
    "        dict_kpss[\"Message11\"]=\"The data is stationary\"\n",
    "        return out_dict,dict_kpss,dict_1\n",
    "        \n",
    "    elif out1==\"stationary\" and out2==\"non_stationary\":\n",
    "        data[\"diff_1\"] =series.diff(periods=1)\n",
    "        data['diff_1'].dropna()\n",
    "        dict_2={\"Message\":\"KPSS indicates non-stationarity and ADF indicates stationarity - The series is difference stationary.Therefore differencing has been used to make the data stationary.\"}\n",
    "        out_dict=out.to_dict()\n",
    "        out_dict[\"Message1\"]=\"Strong evidence against the null hypothesis\"\n",
    "        out_dict[\"Message2\"]=\"Data has no unit root and is stationary\"\n",
    "        \n",
    "        dict_kpss[\"Message11\"]=\"The data is non-stationary\"\n",
    "        result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data  \n",
    "        labels = ['ADF test statistic','p-value','# lags used','number_of_observations_used']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        return out_dict,dict_kpss,dict_2\n",
    "        \n",
    "    elif out1==\"non_stationary\" and out2==\"stationary\":\n",
    "        \n",
    "        data['data_log']=np.sqrt(series)\n",
    "        data['data_diff']=data['data_log'].diff().dropna()\n",
    "        dict_2={\"Message\":\"KPSS indicates stationarity and ADF indicates non-stationarity - The series is trend stationary. Trend needs to be removed to make series strict stationary.Therefore log transformation has been used to make the data strict stationary.\"}\n",
    "        out_dict=out.to_dict()\n",
    "        out_dict[\"Message1\"]=\"Weak evidence against the null hypothesis\"\n",
    "        out_dict[\"Message2\"]=\"Data has a unit root and is non-stationary\"\n",
    "        dict_kpss[\"Message11\"]=\"The data is stationary\"\n",
    "        return out_dict,dict_kpss,dict_2\n",
    "    elif out1==\"non_stationary\" and out2==\"non_stationary\":\n",
    "        data['data_log']=np.sqrt(series)\n",
    "        data['data_diff']=data['data_log'].diff().dropna()\n",
    "        dict_2={\"Message\":\"Both ADF and KPSS test indicates non-stationarity.Therefore log transformation has been chosen to make the data stationarity\"}\n",
    "        out_dict=out.to_dict()\n",
    "        out_dict[\"Message1\"]=\"Weak evidence against the null hypothesis\"\n",
    "        out_dict[\"Message2\"]=\"Data has a unit root and is non-stationary\"\n",
    "        dict_kpss[\"Message11\"]=\"The data is non-stationary\"\n",
    "        return out_dict,dict_kpss,dict_2\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31c71b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test\n",
    "def adf_test(target_col):\n",
    "\n",
    "    result = adfuller(target_col.dropna(),autolag='AIC') # .dropna() handles differenced data  \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "    if result[1] < 0.05:        \n",
    "        Dict_1={\"message1\":\"Strong evidence against the null hypothesis\"}\n",
    "        Dict_2={\"message2\":\"Reject the null hypothesis\"}\n",
    "        Dict_3={\"message3\":\"Data has no unit root and is stationary\"}\n",
    "    else:\n",
    "        Dict_1={\"message1\":\"Weak evidence against the null hypothesis\"}\n",
    "        Dict_2={\"message2\":\"Fail to reject the null hypothesis\"}\n",
    "        Dict_3={\"message3\":\"Data has a unit root and is non-stationary\"}\n",
    "\n",
    "    return out, Dict_1,Dict_2,Dict_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0b7c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPSS test for stationarity and display output\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(target_col):  \n",
    "    statistic, p_value, n_lags, critical_values = kpss(target_col)\n",
    "    # Format Output\n",
    "    Dict_1={'KPSS Statistic': statistic}\n",
    "    Dict_2={'p-value': p_value}\n",
    "    Dict_3={'num lags': n_lags}\n",
    "    Dict_4={'Critial Values':critical_values}\n",
    "    \n",
    "\n",
    "    return Dict_1,Dict_2,Dict_3,Dict_4,(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f38421d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of non stationarity to stationarity\n",
    "def non_stationarity_stationarity(data,target_col,adf,kpss,choice='T'):    \n",
    "    if adf==\"stationary\" and kpss==\"stationary\":        \n",
    "        Dict_1={\"Message\":\"Data has no unit root and is Stationary\"} \n",
    "        return Dict_1        \n",
    "    elif adf==\"non stationary\" and kpss==\"non stationary\":        \n",
    "        Dict_2={\"Message\":\"The data has been converted into Stationary data successfully!!!\"}\n",
    "        if choice == 'T':  \n",
    "            data['data_log']=np.sqrt(target_col)\n",
    "            data['data_diff']=data['data_log'].diff().dropna()\n",
    "            return Dict_2,adf_test(data['data_diff']); \n",
    "        \n",
    "        elif choice == 'D':\n",
    "            data[\"diff_1\"] =target_col.diff(periods=1)\n",
    "            data['diff_1'].dropna()\n",
    "            return Dict_2,adf_test(data['diff_1']); \n",
    "              \n",
    "    elif adf==\"non stationary\" and kpss==\"stationary\":\n",
    "        data['data_log']=np.sqrt(target_col)\n",
    "        data['data_diff']=data['data_log'].diff().dropna()\n",
    "        return Dict_2,adf_test(data['data_diff']); \n",
    "        \n",
    "    elif adf==\"stationary\" and kpss==\"non stationary\":\n",
    "        \n",
    "        data[\"diff_1\"] =target_col.diff(periods=1)\n",
    "        data['diff_1'].dropna()\n",
    "        return Dict_2,adf_test(data['diff_1']);         \n",
    "    else:\n",
    "        Dict_3={\"Warning\": \"Please enter valid input\"}\n",
    "        return Dict_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72814a08",
   "metadata": {},
   "source": [
    "# ACF PACF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this acf_pacf function \n",
    "def acf_pacf(series,nlags=15,alpha=0.05):\n",
    "    acf, ci = sm.tsa.acf(df1,nlags=15, alpha=0.05)\n",
    "    pacf, ci1 = sm.tsa.pacf(df1,nlags=15, alpha=0.05)\n",
    "    mydict_acf={\n",
    "        'title':'ACF_plot',\n",
    "        'x_label':\"Lags\"\n",
    "        'y_label':\"PACF_values\"\n",
    "        'x_val':[i for i in range(0,nlags+2)],\n",
    "        'y_val_acf':acf, #y values have two components acf values and confidence interval values.\n",
    "        'y_val_confidence_interval':ci,\n",
    "        'type':'stem_plot-aka lollipop plots'}\n",
    "    mydict_pacf={ \n",
    "        \"title\":\"PACF Plot\",\n",
    "        \"x_label\" :'Lags',\n",
    "        \"y_label\" :'PACF_values',\n",
    "        \"x_values\":[j for j in range(0,nlags+2)],\n",
    "        \"y_values\":pacf,\n",
    "        \"y_val_confidence_interval\":ci1\n",
    "        \"Chart_type\":\"stem_plot-aka lollipop plots\"}\n",
    "    \n",
    "    dict1={\"Interpretation\":\"ACF represnts auto correlation between varibles w.r.t Time into consideration all components of time series.PACF represnts correlation function of the variables with residuals partially.\"}\n",
    "    dict2={\"\":\"Both ACF & PACF starts at lag 0 , which is the correlation of variables with itself and therefore results in a correlation of 1. Difference between both is inclusion and exclusion of indirect correlations. Blue area depicts 95% confidence interval.\"}\n",
    "    dict3={ \"Sharp Drop Point\": \n",
    "            [\"Instant drop lag just after lag 0.\",\n",
    "\n",
    "            \"ACF sharp drop point implies MA order & PACF sharp drop point implies AR order\",\n",
    "\n",
    "            \"Some basic approach for model choosing are as follows\",\n",
    "\n",
    "            \"1. ACF plot declines gradually and PACF drops instantly use AR model.\",\n",
    "            \"2. ACF drops instantly and PACF declines gradually use MA model.\", \n",
    "            \"3. Both declines gradually use ARMA model\",\n",
    "            \"4. Both drops instantly we are not able to model the time series.\"]}\n",
    "\n",
    "    dict4={\"Note\":\n",
    "\n",
    "            \"ARIMA and SARIMA models are Intergrated ARMA models we will use the same identified orders from both the plots.\"}\n",
    "    return mydict_acf,mydict_pacf,dict1,dict2,dict3,dict4\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1962c",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting dataset\n",
    "def split(data,size_input=0.75):\n",
    "    #size_input=float(input(\"Please enter the size of percentage where you want to split the data-for eg 0.75 for 75% or 0.80 for 80%\"))\n",
    "    #splitting 85%/15% because of little amount of data\n",
    "    size = int(len(data) * size_input)\n",
    "    train= data[:size]\n",
    "    test = data[size:]\n",
    "    return(train,test)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test= split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4fcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb124dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80547a",
   "metadata": {},
   "source": [
    "# Forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ccaa0",
   "metadata": {},
   "source": [
    "## AutoArima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40221429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoarima model\n",
    "def gen_auto_arima(df, col, m, f, periods, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5):\n",
    "    automodel= auto_arima(df[col], seasonal=True, m=m, start_p=0, start_q=0, d=None, D=None, stepwise=True, max_p= maxp, max_d= maxd, max_q = maxq,\n",
    "                         max_P= maxP, max_D= maxD, max_Q= maxQ)\n",
    "    #print(automodel.summary())\n",
    "    preds, confint = automodel.predict(n_periods=periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1], periods = periods, freq=f)\n",
    "    fitted_series = pd.DataFrame(preds, index=index_of_fc)\n",
    "    \n",
    "    #print(preds)\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Auto ARIMA Model\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Auto_ARIMA_Forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":fitted_series, # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col],fitted_series)).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-fitted_series)/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Auto ARIMA Model'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec71cc",
   "metadata": {},
   "source": [
    "### Simpler timeseries models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e060cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_method(test_df):\n",
    "    y_hat_naive = test_df.copy()\n",
    "    y_hat_naive['naive_forecast'] = train[target_col][train_len-1]\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Naive Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Naive_Forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_naive['naive_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_naive['naive_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_naive['naive_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4e5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_method(test_df):\n",
    "    y_hat_average = test_df.copy()\n",
    "    y_hat_average['average_forecast'] = train[target_col].mean()\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Average Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Average_Forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_average['average_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_average['average_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_average['average_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Average method'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return mydict,results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4f56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_moving_average(df, ma_window):\n",
    "    y_hat_sma = df.copy()\n",
    "    y_hat_sma['sma_forecast'] = data[target_col].rolling(ma_window).mean()\n",
    "    y_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Simple Moving Average Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Simple_Moving_Average_Forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_sma['sma_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_sma['sma_forecast'][train_len:])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_sma['sma_forecast'][train_len:])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6500a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_exponential_smoothing(test_df,forecast_duration):\n",
    "    model = SimpleExpSmoothing(train[target_col])\n",
    "    model_fit = model.fit(smoothing_level=0.2,optimized=False)\n",
    "    model_fit.params\n",
    "    y_hat_ses = test_df.copy()\n",
    "    y_hat_ses['ses_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Simple Exponential Smoothing Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Simple exponential smoothing forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_ses['ses_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_ses['ses_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_ses['ses_forecast'])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebe172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoltExponentialSmoothing(train_df,test_df,seasonal_periods,forecast_duration):\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='additive', seasonal=None)\n",
    "    model_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\n",
    "    print(model_fit.params)\n",
    "    y_hat_holt = test_df.copy()\n",
    "    y_hat_holt['holt_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Holt\\'s Exponential Smoothing Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Holt\\'s exponential smoothing forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_holt['holt_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_holt['holt_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_holt['holt_forecast'])/test['Passengers'])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da478f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Holtwinter_exponentialsmoothing_additive(train_df,test_df,seasonal_periods,forecast_duration):    \n",
    "    y_hat_hwa = test.copy()\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='add', seasonal='add')\n",
    "    model_fit = model.fit(optimized=True)\n",
    "    y_hat_hwa['hw_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Holt Winters\\' Additive Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Holt Winters\\'s additive forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_hwa['hw_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test['Passengers'], y_hat_hwa['hw_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test['Passengers']-y_hat_hwa['hw_forecast'])/test['Passengers'])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5244cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Holtwinter_exponentialsmoothing_multiplicative(train_df,test_df,seasonal_periods,forecast_duration):    \n",
    "    y_hat_hwa = test.copy()\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='add', seasonal='mul')\n",
    "    model_fit = model.fit(optimized=True)\n",
    "    y_hat_hwa['hw_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Holt Winters\\' multiplicative Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Holt Winters\\'s multiplicative forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_hwa['hw_forecast'], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_hwa['hw_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_hwa['hw_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018ed176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_function(data,train_df,test_df):\n",
    "    data_boxcox = pd.Series(boxcox(data[target_col], lmbda=0), index = data.index)\n",
    "    data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), data.index)\n",
    "    train_data_boxcox = data_boxcox[:train_len]\n",
    "    test_data_boxcox = data_boxcox[train_len:]\n",
    "    train_data_boxcox_diff = data_boxcox_diff[:train_len-1]\n",
    "    test_data_boxcox_diff = data_boxcox_diff[train_len-1:]\n",
    "    model = ARIMA(train_data_boxcox_diff, order=(1, 0, 0)) \n",
    "    model_fit = model.fit()\n",
    "    y_hat_ar = data_boxcox_diff.copy()\n",
    "    y_hat_ar['ar_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\n",
    "    y_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox_diff'].cumsum()\n",
    "    y_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox'].add(data_boxcox[2])\n",
    "    y_hat_ar['ar_forecast'] = np.exp(y_hat_ar['ar_forecast_boxcox'])\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Auto Regression Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Auto regression forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_ar['ar_forecast'][test.index.min():], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_ar['ar_forecast'][test.index.min():])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_ar['ar_forecast'][test.index.min():])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Autoregressive (AR) method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return mydict,results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3318ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_function(data,train_df,test_df):\n",
    "    data_boxcox = pd.Series(boxcox(data[target_col], lmbda=0), index = data.index)\n",
    "    data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), data.index)\n",
    "    train_data_boxcox = data_boxcox[:train_len]\n",
    "    test_data_boxcox = data_boxcox[train_len:]\n",
    "    train_data_boxcox_diff = data_boxcox_diff[:train_len-1]\n",
    "    test_data_boxcox_diff = data_boxcox_diff[train_len-1:]\n",
    "    model = ARIMA(train_data_boxcox_diff, order=(0, 0, 1)) \n",
    "    model_fit = model.fit()\n",
    "    y_hat_ar = data_boxcox_diff.copy()\n",
    "    y_hat_ar['ma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\n",
    "    y_hat_ar['ma_forecast_boxcox'] = y_hat_ar['ma_forecast_boxcox_diff'].cumsum()\n",
    "    y_hat_ar['ma_forecast_boxcox'] = y_hat_ar['ma_forecast_boxcox'].add(data_boxcox[2])\n",
    "    y_hat_ar['ma_forecast'] = np.exp(y_hat_ar['ma_forecast_boxcox'])\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Moving Average Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Moving Average forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_ar['ma_forecast'][test.index.min():], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_ar['ma_forecast'][test.index.min():])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_ar['ma_forecast'][test.index.min():])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Moving Average (MA) method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f47f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_moving_average_function(data,train_df,test_df):\n",
    "    data_boxcox = pd.Series(boxcox(data[target_col], lmbda=0), index = data.index)\n",
    "    data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), data.index)\n",
    "    train_data_boxcox = data_boxcox[:train_len]\n",
    "    test_data_boxcox = data_boxcox[train_len:]\n",
    "    train_data_boxcox_diff = data_boxcox_diff[:train_len-1]\n",
    "    test_data_boxcox_diff = data_boxcox_diff[train_len-1:]\n",
    "    model = ARIMA(train_data_boxcox_diff, order=(1, 0, 1)) \n",
    "    model_fit = model.fit()\n",
    "    y_hat_ar = data_boxcox_diff.copy()\n",
    "    y_hat_ar['arma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\n",
    "    y_hat_ar['arma_forecast_boxcox'] = y_hat_ar['arma_forecast_boxcox_diff'].cumsum()\n",
    "    y_hat_ar['arma_forecast_boxcox'] = y_hat_ar['arma_forecast_boxcox'].add(data_boxcox[2])\n",
    "    y_hat_ar['arma_forecast'] = np.exp(y_hat_ar['arma_forecast_boxcox'])\n",
    "    \n",
    "    mydict={ \n",
    "        \"title\":\"Autoregressive Moving Average Method\",\n",
    "        \"x_label\" :'Date',\n",
    "        \"y_label\" :'Values',\n",
    "        \"legends\":['Train_data','Test_data','Auto regression moving average forecast'],\n",
    "        \"x_values\":Date,\n",
    "        \"y1_values\":train[target_col], # upto this line in blue colour\n",
    "        \"y2_values\":test[target_col], # this line in orange colour\n",
    "        \"y3_values\":y_hat_ar['arma_forecast'][test.index.min():], # this line in green colour\n",
    "        \"Chart_type\":\"line plot\"}\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_ar['arma_forecast'][test.index.min():])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_ar['arma_forecast'][test.index.min():])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Autoregressive Moving Average (ARMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return mydict,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a072bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956ef0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914238bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb283f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249d6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d1e45cadc3597bb8b6600530fbdf8c3eefe919a24ef54d9d32b318795b772e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
