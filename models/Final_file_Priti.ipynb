{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Todo: To be removed\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import chart_studio.plotly as ply\n",
    "import cufflinks as cf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cabab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cufflinks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9679969",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing CSV Dataset\n",
    "#file = r\"C:\\Users\\plahare\\Downloads\\BeerWineLiquor.csv\"\n",
    "df = pd.read_csv(\"BeerWineLiquor.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4e93c",
   "metadata": {},
   "source": [
    "# Setting Target col and Date col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropdown for selecting target column for forecasting and date column \n",
    "target_col = 'beer'\n",
    "ts_col = 'date'\n",
    "# Drop down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee546c6",
   "metadata": {},
   "source": [
    "# Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing date to datetime and set it as an index\n",
    "def setIndex(data):\n",
    "    data[ts_col] = pd.to_datetime(data[ts_col])\n",
    "    data.set_index(ts_col,inplace=True)\n",
    "    print(data.head())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5565e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "setIndex(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225d60c",
   "metadata": {},
   "source": [
    "# Shape of Data / Rows & Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying rows and columns\n",
    "def Shape_df(data):\n",
    "    s=data.shape\n",
    "    print('No of rows :{}'.format(s[0]))\n",
    "    print('No of Columns:{}'.format(s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8694a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Shape_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86335a6e",
   "metadata": {},
   "source": [
    "# Head & Tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b14cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving choice to user to display head or tail \n",
    "# Todo: Remove choice\n",
    "def display_head_tail(data, choice='Head'):\n",
    "    if choice == 'Head':\n",
    "        return data.head()\n",
    "    elif choice=='Tail':\n",
    "        return data.tail()\n",
    "    else:\n",
    "        return {\"message\": \"Invalid choice.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_head_tail(df)\n",
    "display_head_tail(df, 'Head')\n",
    "display_head_tail(df, 'Tail')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998034f",
   "metadata": {},
   "source": [
    "# Describe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display descriptive statistics \n",
    "def Describe_data(df):\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d081cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37354b95",
   "metadata": {},
   "source": [
    "# Resampling  Countinous/Discontinous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e408cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= df.asfreq(pd.infer_freq(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd351be1",
   "metadata": {},
   "source": [
    "# Team is working on this will update you \n",
    "def check_Continuity(data):\n",
    "    c=pd.infer_freq(data.index)\n",
    "    if c==None:\n",
    "        print(\"This is non-continuous data\")\n",
    "        #Function for Resampling\n",
    "    else:\n",
    "        print(\"This is continuous data \")\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be443d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_Continuity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb51400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840fc54a",
   "metadata": {},
   "source": [
    "# Null Value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0f9d6",
   "metadata": {},
   "source": [
    "## List of columns having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This functions creates a dictionary where columns are keys and values are percentage of null values present in that column \n",
    "def null_list(df):\n",
    "    \n",
    "    mydict={}#an empty dictionary for storing null value percentage\n",
    "    list1=[]\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().sum()>0: #this is to create a dictionary with columns which has null values.\n",
    "            mydict[i]=[(df.isnull().sum())*100 / len(df)][0][i]\n",
    "    for j,k in mydict.items():\n",
    "        list1.append(j)\n",
    "    \n",
    "    if len(list1)==0: \n",
    "        return {\"message\": \"This dataset doesn't have any null values, kindly proceed with the EDA.\"}\n",
    "        # print(\"This dataset doesn't have any null values , kindly proceed with the EDA .\\n\")\n",
    "    else:\n",
    "        return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109cc52",
   "metadata": {},
   "source": [
    "## Graph to display percentage of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting the null values. this function plots graph of columns in the x-axis and its percentage of null values in the y-axis\n",
    "def graph(df):\n",
    "    \n",
    "    null_percentage=(df.isnull().sum() *100)/len(df)\n",
    "    x=np.array(df.columns)\n",
    "    y=np.array(null_percentage)\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    \n",
    "    # set orientation for X axis labels\n",
    "    plt.xticks(rotation=70)\n",
    "\n",
    "    {\n",
    "        'xlabel': 'Columns',\n",
    "        'ylabel': 'Percentage',\n",
    "        'graphTitle': 'Percentage of null values present in each column',\n",
    "        'nullPerrcentage': null_percentage,\n",
    "        'xData': x,\n",
    "        'yData': y,\n",
    "        'graphType': 'bar'\n",
    "    }\n",
    "\n",
    "# draw bar chart\n",
    "    plt.bar(x,y)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49715941",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25157e",
   "metadata": {},
   "source": [
    "## Null Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b003dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe as an input and returns a dictionary with column names and null %\n",
    "def get_null_percentages(df):\n",
    "    mydict={}\n",
    "\n",
    "    for key in df.columns:\n",
    "        mydict[key] = [(df.isnull().sum())*100 / len(df)][0][key]\n",
    "    \n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe and column name as an input.\n",
    "def drop_rows(df, col_name):\n",
    "    return df.dropna(subset=[col_name], axis=0, how=\"any\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66003f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe and column name as an input.\n",
    "def drop_cols(df, col_name):\n",
    "    return df.drop([col_name],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe, column name, and impute method as an input.\n",
    "def impute(df, col_name, impute_method='interpolation'):\n",
    "\n",
    "    if df.dtypes[col_name] == str or df.dtypes[col_name] == object:\n",
    "        return df[col_name].fillna(df[col_name].mode()[0], inplace=True)\n",
    "\n",
    "    else:\n",
    "        flag1 = (df[col_name].isnull() & df[col_name].shift(-1).isnull()).any()\n",
    "        flag2 = df[col_name].head(1).isnull().bool()\n",
    "        flag3 = df[col_name].tail(1).isnull().bool()\n",
    "\n",
    "        if flag1 or flag2 or flag3:\n",
    "            return df[col_name].fillna(df[col_name].interpolate(method='linear', limit_direction=\"both\"), inplace=True)\n",
    "\n",
    "        elif impute_method == \"locf\" and (flag1 == False and flag2 == False and flag3 == False):\n",
    "            return df[col_name].fillna(df[col_name].ffill(), inplace=True)\n",
    "            \n",
    "        elif impute_method == \"nocb\" and (flag1 == False and flag2 == False and flag3 == False):\n",
    "            return df[col_name].fillna(df[col_name].bfill(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_choices = ['drop_rows', 'drop_cols', 'impute']\n",
    "\n",
    "# def treat_nulls(df, col_name, choice='drop_rows'):\n",
    "\n",
    "#     if df and col_name and (choice in valid_choices):\n",
    "\n",
    "#         if choice == valid_choices[0]:\n",
    "#             return drop_rows(df, col_name)\n",
    "\n",
    "#         elif choice ==  valid_choices[1]:\n",
    "#             return drop_cols(df, col_name)\n",
    "\n",
    "#         elif choice ==  valid_choices[2]:\n",
    "#             return impute(df, col_name)\n",
    "            \n",
    "#         else:\n",
    "#             return {\"message\": \"Invalid choice.\"}    \n",
    "\n",
    "#     else:\n",
    "#         return {\"message\": \"Please provide dataframe and column name to process.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b1dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function iterates through whole dataframe and treats the missing values with appropriate method chosen by the user.\n",
    "def null_values(df, choice = 'drop_rows', listOfCols = []):\n",
    "\n",
    "    #an empty dictionary for storing the null values and its percentage\n",
    "    mydict={}\n",
    "\n",
    "    for key in df.columns:\n",
    "        mydict[key] = [(df.isnull().sum())*100 / len(df)][0][key]\n",
    "    \n",
    "    '''\n",
    "\n",
    "        {\n",
    "            'col1': 1,\n",
    "            'col2': 0, \n",
    "            'col3': 10,\n",
    "            'col4': 100\n",
    "        }\n",
    "\n",
    "    ''' \n",
    "\n",
    "    #looping through the whole dataframe using dictionary\"mydict\"\n",
    "    # Note: key refers to the column name, and value refers to its NULL %\n",
    "    for key, value in mydict.items():\n",
    "        \n",
    "        # if column has 0% null values, then ignore it. else do something to that column.\n",
    "        if value==0:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            flag=True\n",
    "\n",
    "            while flag:\n",
    "                choice=input(\"Kindly choose whether you want to opt for dropping the rows/columns or would like to impute the values? Please type 'drop_rows' for dropping the rows or 'drop_column' for dropping the columns and 'impute' for filling the missing values\\n\")\n",
    "                \n",
    "                if choice==\"drop_rows\" or choice==\"drop_column\" or choice==\"impute\":\n",
    "                    flag=False\n",
    "                else :\n",
    "                    print(\"enter a valid choice\")\n",
    "\n",
    "            #if user chooses to drop the rows, then it will perform the following operation\n",
    "            if choice==\"drop_rows\":\n",
    "                df.dropna(subset=[key],axis=0,how=\"any\",inplace=True)\n",
    "                \n",
    "                mydict1={}\n",
    "                for key in df.columns:\n",
    "                    mydict1[key]=[(df.isnull().sum())*100 / len(df)][0][key]\n",
    "                mydict.update(mydict1)\n",
    "\n",
    "\n",
    "             #if user chooses to drop the column, then it will perform the following operation   \n",
    "            elif choice==\"drop_column\":\n",
    "                df.drop([key],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "            #if user chooses to impute the missing values, then it will perform the following operation\n",
    "            elif choice==\"impute\":\n",
    "\n",
    "                if df.dtypes[key]==str or df.dtypes[key]==object:\n",
    "                    df[key].fillna(df[key].mode()[0], inplace=True)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    boolean=(df[key].isnull() & df[key].shift(-1).isnull()).any()\n",
    "                    boolean1=df[key].head(1).isnull().bool()\n",
    "                    boolean2=df[key].tail(1).isnull().bool()\n",
    "\n",
    "                    if boolean==True or boolean1==True or boolean2==True:\n",
    "                        df[key].fillna(df[key].interpolate(method='linear',limit_direction=\"both\"),inplace=True)\n",
    "\n",
    "                        \n",
    "                    else:\n",
    "                        Flag1=True\n",
    "                        \n",
    "                        while Flag1:\n",
    "                            impute=input(\"Kindly Choose any one method for imputing missing values - please type 'LOCF' or 'NOCB'  or 'Interpolation'.\\n\")\n",
    "\n",
    "                            if impute==\"LOCF\" or impute==\"NOCB\" or impute==\"Interpolation\":\n",
    "                                Flag1=False\n",
    "                            else:\n",
    "                                print(\"enter a valid input\")\n",
    "                        \n",
    "                        if impute==\"LOCF\":\n",
    "                            df[key].fillna(df[key].ffill(),inplace=True)\n",
    "                        elif impute==\"NOCB\":\n",
    "                            df[key].fillna(df[key].bfill(),inplace=True)\n",
    "                        elif impute==\"Interpolation\":\n",
    "                            df[key].fillna(df[key].interpolate(method='linear',limit_direction=\"both\"),inplace=True)\n",
    "                        \n",
    "                            \n",
    "            \n",
    "                \n",
    "    print(df.isnull().sum())\n",
    "    print(\"The null values have been successfully treated!\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cc0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e8595",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c283b58",
   "metadata": {},
   "source": [
    "## Date vs target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c50f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full plot of target column\n",
    "def Plot_col(df,col_name):\n",
    "    title = '{}'.format(col_name)\n",
    "    df[target_col].plot(figsize=(12,6),title=title).autoscale(axis='both',tight=True)\n",
    "    print(\"Interpretation:\\n This graph represents visualization of dependent or target variable w.r.t Time.This depicts how the dependent variable varies with the time. X axis represents time and Y axis represents dependent variable. \")\n",
    "    #df.plot(ts_col,col_name,figsize=(12,6),title=title).autoscale(axis='both',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_col(df,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd463982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an interactive plot of above code using plotly\n",
    "def plotly_line(data,col_name):\n",
    "    fig= px.line(x=data.index,y=data[col_name])\n",
    "    fig.show()\n",
    "    print(\"Interpretation:\\n This graph represents visualization of dependent or target variable w.r.t Time.This depicts how the dependent variable varies with the time. X axis represents time and Y axis represents dependent variable. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2271699",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_line(df,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ab4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to show the alias image to the user\n",
    "from IPython.display import Image\n",
    "Image(filename=\"C:\\\\Users\\\\DB4\\\\Downloads\\\\MicrosoftTeams-image.png\",width=1000,height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5e585",
   "metadata": {},
   "source": [
    "## Resampled plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07353c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampled plot as per aliases input by user\n",
    "def resample_plot(data,col_name):\n",
    "    title = 'Resampled {} graph'.format(col_name)\n",
    "    \n",
    "    resample_alias = input(\"Please enter an offset alias: \")\n",
    "    data[col_name].resample(resample_alias).max().plot.bar(figsize=(16,6), title=title);\n",
    "    print(\"Interpretation:\\n Resampling:\\n Conversion of frequency of time in time series data. \\nThis graph represents visualization of resampled dependent or target variable w.r.t Time.This depicts how the resampled dependent variable varies with the time. X axis represents resampled or extended time and Y axis represents dependent variable. Main use of this plot is to show how the data behaves with different frequencies. This deals with the missing dates also, so it helps to make data continuous. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1136eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_plot(df,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f002d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly function for resampled plot\n",
    "def Plotly_bar_resample(data,col_name):\n",
    "    resample_alias = input(\"Please enter an offset alias: \")\n",
    "    fig=px.bar(data[col_name].resample(resample_alias).max(),y=col_name,color=col_name,color_continuous_scale=px.colors.sequential.Aggrnyl_r)\n",
    "    fig.update_layout(title_text='Resampled {} graph'.format(col_name))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotly_bar_resample(df,target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce986d",
   "metadata": {},
   "source": [
    "## Top n Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying top n values in dataframe\n",
    "def top_n_values(data,col_name):\n",
    "    n = int(input(\"How many top values do you want to see?\\n\"))\n",
    "    print(\"Below are the top {0} values in the {1} column: \".format(n, col_name))\n",
    "    return pd.DataFrame(data[col_name].sort_values(ascending = False).head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_n_dataf =top_n_values(df,target_col)\n",
    "top_n_dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ab81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot for visualizing top n values\n",
    "def plot_top_n(data,col_name):\n",
    "    n= len(top_n_dataf)\n",
    "    fig = px.bar(data, x=data.index, y=col_name, labels={'x':'{}'.format(col_name)},\n",
    "             color=col_name, color_continuous_scale=px.colors.sequential.Brwnyl)\n",
    "    fig.update_layout(title_text='Top {} {} graph'.format(n,col_name))\n",
    "    fig.show()\n",
    "    print(\"Interpretation:\\n This graph represents visualization of Top values of dependent or target variable w.r.t Time. X axis represents time and Y axis represents top values of dependent variable. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n(top_n_dataf,target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861a7c8",
   "metadata": {},
   "source": [
    "# Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43d2c7",
   "metadata": {},
   "source": [
    "## Seasonal Decompose during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal decomposition plot\n",
    "def decomposition(series):\n",
    "    \n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    from dateutil.parser import parse\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "   \n",
    "    \n",
    "\n",
    "    plt.rcParams.update({'figure.figsize': (20,10)})\n",
    "    y = series.to_frame()\n",
    "    \n",
    "    choice=input(\"Enter 'M' for Multiplicative decomposition & 'A' for Additive decomposition & 'MA' for both :\\n\")\n",
    "    \n",
    "    if choice == 'M':\n",
    "        \n",
    "    # Multiplicative Decomposition \n",
    "       seasonal_decompose(y, model='multiplicative',period = 52).plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "    elif choice == 'A':\n",
    "        \n",
    "    # Additive Decomposition\n",
    "       seasonal_decompose(y, model='additive',period = 52).plot().suptitle('Additive Decompose', fontsize=22);\n",
    "    elif choice==\"MA\":\n",
    "        seasonal_decompose(y, model='multiplicative',period = 52).plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "        \n",
    "        seasonal_decompose(y, model='additive',period = 52).plot().suptitle('Additive Decompose', fontsize=22)\n",
    "    else:\n",
    "        print(\" This is invalid choice. Please choose Either M or A\")\n",
    "    print(\"\"\"Interpretation:\\n Here X axis represents Time and Y axis represents Normal scaled data. Time series has 4 components Trend,seasonality,cyclical variation and irregular variation. \\n Trend component: This is useful in predicting future movements. Over a long period of time, the trend shows whether the data tends to increase or decrease. \\n \n",
    "            Seasonal component: The seasonal component of a time series is the variation in some variable due to some predetermined patterns in its behavior. \\n Cyclical component: The cyclical component in a time series is the part of the movement in the variable which can be explained by other cyclical movements in the economy. \\n  irregular component: this term gives information about non-seasonal patterns.\\n\n",
    "            \\nTime series has two types of decomposition models Additive Model and Multiplicative model. The plot shows the decomposition of your time series data in its seasonal component, its trend component and the remainder. If you add or multiply the decomposition together you would get back the actual data. First block represents original series , second represents trend , third represents seasonality presents, fourth represents error component or residual. \n",
    "            \\nFor additive if we add below three blocks we get original data series. Similarly for multiplicative we have to multiply the components. \"\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition(df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b486b",
   "metadata": {},
   "source": [
    "## Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755af122",
   "metadata": {},
   "source": [
    "## Stationarity Check Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for checking stationarity\n",
    "def stationarity_check_plot(timeseries,col_name):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation for {}'.format(col_name))\n",
    "    plt.show(block=False)\n",
    "    print(\"Interpretation:\\n\\n Stationarity:\\n\\n Stationarity means that the statistical properties of a process generating a time series do not change over time. That is Mean and Standard deviation is approximately constant over time.\\n\\nStationarity Graph represents stationarity of the series w.r.t. Time. X axis depicts time and Y axis depicts Dependent variable . Blue line represents the original Time series data , Red line represents Mean of the series data and Black line represents standard deviation of the series. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check_plot(df,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adf test for checking stationarity and display output to user\n",
    "def adf_test(series):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "   \n",
    "    print(f'Augmented Dickey-Fuller Test: ')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string(), '\\n')          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")\n",
    "        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(df[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPSS test for stationarity and display output\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(series):  \n",
    "    statistic, p_value, n_lags, critical_values = kpss(series)\n",
    "    # Format Output\n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critial Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38421d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of non stationarity to stationarity\n",
    "def non_stationarity_stationarity(data,series):\n",
    "    adf=input(\"Enter result of adf test either stationary or non stationary :\\n\")\n",
    "    kpss=input(\"Enter result of kpss test either stationary or non stationary :\\n\")\n",
    "    \n",
    "    if adf==\"stationary\" and kpss==\"stationary\":\n",
    "        \n",
    "        print(\"Data has no unit root and is Stationary\")\n",
    "        \n",
    "    elif adf==\"non stationary\" and kpss==\"non stationary\":\n",
    "        \n",
    "        print(\"Data has unit root and is non stationary, please make data stationary\")\n",
    "        \n",
    "        choice=input(\"Enter T for Transformation method or D for differencing method :\\n\")\n",
    "    \n",
    "        if choice == 'T':\n",
    "            data['data_log']=np.sqrt(series)\n",
    "            data['data_diff']=data['data_log'].diff().dropna()\n",
    "            adf_test(data['data_diff']); \n",
    "        \n",
    "        elif choice == 'D':\n",
    "            data[\"diff_1\"] =series.diff(periods=1)\n",
    "            data['diff_1'].dropna()\n",
    "            adf_test(data['diff_1']); \n",
    "        \n",
    "        else:\n",
    "            print(\" This is invalid choice. Please choose Either T or D\")\n",
    "        \n",
    "    elif adf==\"non stationary\" and kpss==\"stationary\":\n",
    "        data['data_log']=np.sqrt(series)\n",
    "        data['data_diff']=data['data_log'].diff().dropna()\n",
    "        adf_test(data['data_diff']); \n",
    "        \n",
    "    elif adf==\"stationary\" and kpss==\"non stationary\":\n",
    "        \n",
    "        data[\"diff_1\"] =series.diff(periods=1)\n",
    "        data['diff_1'].dropna()\n",
    "        adf_test(data['diff_1']); \n",
    "        \n",
    "    else:\n",
    "        print(\"Please enter valid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stationarity_stationarity(df,df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72814a08",
   "metadata": {},
   "source": [
    "# ACF PACF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for ACF and PACF\n",
    "def ACF_PACF(series):\n",
    "\n",
    "\n",
    "\n",
    "    choice=input(\"Ideal Choice for lags are considered to be 10% to 30% of the length of the data.That means lags between 10 to 30 might be used, Please choose Accordingly.:\\n\")\n",
    "    lags=int(choice)\n",
    "    plt.rcParams.update({'figure.figsize': (20,6)})\n",
    "\n",
    "    sm.graphics.tsa.plot_acf(series, lags=lags,title='auto correlation ',zero=False);\n",
    "    sm.graphics.tsa.plot_pacf(series, lags=lags,title='partial auto correlation ',zero=False);\n",
    "    print(\"Interpretation : \\n \")\n",
    "    print(\"\"\"ACF represnts auto correlation between varibles w.r.t Time into consideration all components of time series.PACF represnts correlation function of the variables with residuals partially . \\n\"\"\")\n",
    "    print(\"Both ACF & PACF starts at lag 0 , which is the correlation of variables with itself and therefore results in a correlation of 1. Difference between both is inclusion and exclusion of indirect correlations. Blue area depicts 95% confidence interval.\\n\")\n",
    "    print(\"CONCLUSION:\\n\")\n",
    "    print( \"\"\" Sharp Drop Point: \n",
    "            Instant drop lag just after lag 0.\n",
    "\n",
    "            ACF sharp drop point implies MA order & PACF sharp drop point implies AR order \n",
    "\n",
    "            Some basic approach for model choosing are as follows:\n",
    "\n",
    "            1. ACF plot declines gradually and PACF drops instantly use AR model.\n",
    "            2. ACF drops instantly and PACF declines gradually use MA model. \n",
    "            3. Both declines gradually use ARMA model\n",
    "            4. Both drops instantly we are not able to model the time series.\n",
    "\n",
    "            Note:\n",
    "\n",
    "            ARIMA and SARIMA models are Intergrated ARMA models we will use the same identified orders from both the plots.\n",
    "\n",
    "\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACF_PACF(df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1962c",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting dataset\n",
    "def split(data):\n",
    "    size_input=float(input(\"Please enter the size of percentage where you want to split the data-for eg 0.75 for 75% or 0.80 for 80%\"))\n",
    "    #splitting 85%/15% because of little amount of data\n",
    "    size = int(len(data) * size_input)\n",
    "    train= data[:size]\n",
    "    test = data[size:]\n",
    "    return(train,test)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test= split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76607aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()#ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4fcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape#ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb124dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape#ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e8eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701d372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec80547a",
   "metadata": {},
   "source": [
    "# Forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ccaa0",
   "metadata": {},
   "source": [
    "## AutoArima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40221429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoarima model\n",
    "def gen_autoArima(df, col, m, f, periods, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5):\n",
    "    automodel= auto_arima(df[col], seasonal=True, m=m, start_p=0, start_q=0, d=None, D=None, stepwise=True, max_p= maxp, max_d= maxd, max_q = maxq,\n",
    "                         max_P= maxP, max_D= maxD, max_Q= maxQ)\n",
    "    print(automodel.summary())\n",
    "    preds, confint = automodel.predict(n_periods=periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1], periods = periods, freq=f)\n",
    "    fitted_series = pd.Series(preds, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "    print(preds)\n",
    "    plt.plot(df[target_col])\n",
    "    plt.plot(fitted_series, color='darkgreen')\n",
    "    fitted_series.to_excel('Output_forecast.xlsx')\n",
    "    plt.fill_between(lower_series.index,\n",
    "                 lower_series,\n",
    "                 upper_series,\n",
    "                 color='k', alpha=.15)\n",
    "    plt.savefig('Forecast_autoARIMA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_autoArima(train, target_col, 1, 'M', 98, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_autoArima_plotly(df, col, m, f, periods, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5):\n",
    "    automodel= auto_arima(df[col], seasonal=True, m=m, start_p=0, start_q=0, d=None, D=None, stepwise=True, max_p= maxp, max_d= maxd, max_q = maxq,\n",
    "                         max_P= maxP, max_D= maxD, max_Q= maxQ)\n",
    "    print(automodel.summary())\n",
    "    preds, confint = automodel.predict(n_periods=periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1], periods = periods, freq=f)\n",
    "    fitted_series = pd.Series(preds, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "    print(preds)\n",
    "    fitted_series.to_excel('Output_forecast_plotly.xlsx')\n",
    "    fitted_dataframe=pd.DataFrame(fitted_series,index=index_of_fc)\n",
    "    pd.concat([train[target_col],fitted_dataframe],axis=1).iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985d023",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen_autoArima_plotly(train, target_col, 1, 'M', 49, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86101c79",
   "metadata": {},
   "source": [
    "## Functions for simpler models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a57860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for naive model\n",
    "def naive_method(test_df):\n",
    "    y_hat_naive = test_df.copy()\n",
    "    y_hat_naive['naive_forecast'] = train[target_col][train_len-1]\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Naive Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_naive['naive_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_naive['naive_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average method\n",
    "def average_method(test_df):\n",
    "    y_hat_average = test_df.copy()\n",
    "    y_hat_average['average_forecast'] = train[target_col].mean()\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_average['average_forecast'], label='Average forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Average Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_average['average_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_average['average_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Average method'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45226ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for Simple Moving average model\n",
    "def simple_moving_average(df, ma_window):\n",
    "    y_hat_sma = df.copy()\n",
    "    y_hat_sma['sma_forecast'] = data[target_col].rolling(ma_window).mean()\n",
    "    y_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_sma['sma_forecast'], label='Simple moving average forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Simple Moving Average Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_sma['sma_forecast'][train_len:])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_sma['sma_forecast'][train_len:])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for simple exponential smoothing model\n",
    "def simple_exponential_smoothing(test_df,forecast_duration):\n",
    "    model = SimpleExpSmoothing(train[target_col])\n",
    "    model_fit = model.fit(smoothing_level=0.2,optimized=False)\n",
    "    model_fit.params\n",
    "    y_hat_ses = test_df.copy()\n",
    "    y_hat_ses['ses_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_ses['ses_forecast'], label='Simple exponential smoothing forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Simple Exponential Smoothing Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_ses['ses_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_ses['ses_forecast'])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for holt exponential smoothing model\n",
    "def HoltExponentialSmoothing(test_df,seasonal_periods,forecast_duration):\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='additive', seasonal=None)\n",
    "    model_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\n",
    "    print(model_fit.params)\n",
    "    y_hat_holt = test_df.copy()\n",
    "    y_hat_holt['holt_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot( train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Holt\\'s Exponential Smoothing Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_holt['holt_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_holt['holt_forecast'])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd940e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for Holtwinter exp smoothing additive model\n",
    "def Holtwinter_exponentialsmoothing_additive(test_df,seasonal_periods,forecast_duration):    \n",
    "    y_hat_hwa = test.copy()\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='add', seasonal='add')\n",
    "    model_fit = model.fit(optimized=True)\n",
    "    y_hat_hwa['hw_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot( train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Holt Winters\\' Additive Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_hwa['hw_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_hwa['hw_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for holtwinter exp smoothing multiplicative method\n",
    "def Holtwinter_exponentialsmoothing_multiplicative(test_df,seasonal_periods,forecast_duration):    \n",
    "    y_hat_hwa = test.copy()\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='add', seasonal='mul')\n",
    "    model_fit = model.fit(optimized=True)\n",
    "    y_hat_hwa['hw_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot( train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s multiplicative forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Holt Winters\\' multiplicative Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_hwa['hw_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_hwa['hw_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060cdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e5143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe172d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da478f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ed176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318ff85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47f2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a072bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956ef0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdfdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914238bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb283f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249d6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3511b741edf358ae889154f3bc495c260fa1ae46785c1045b514d11573e214b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
