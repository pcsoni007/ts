{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "from dateutil.parser import parse\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# To do remove the graph related libraries .\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#import seaborn as sns\n",
    "#import plotly.express as px\n",
    "#import chart_studio.plotly as ply\n",
    "#import cufflinks as cf\n",
    "#import cufflinks as cf\n",
    "#cf.go_offline()\n",
    "#cf.set_config_file(offline=False, world_readable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f66dac",
   "metadata": {},
   "source": [
    "Need output in form of dataframe, dictinary or json for graph related functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9679969",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing CSV Dataset\n",
    "file = r\"C:\\Users\\plahare\\Downloads\\BeerWineLiquor.csv\"\n",
    "#create function that takes a file for reading and creating dataframe\n",
    "def file_read(data):\n",
    "    return pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4e93c",
   "metadata": {},
   "source": [
    "# Setting Target col and Date col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f139c3a",
   "metadata": {},
   "source": [
    "create functions for target column and index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropdown for selecting target column for forecasting and date column \n",
    "def target_col(col_name):\n",
    "    target_column=data[col_name]\n",
    "    print(target_column.head())\n",
    "    \n",
    "#def ts_col() = 'date'\n",
    "# Drop down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee546c6",
   "metadata": {},
   "source": [
    "# Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing date to datetime and set it as an index\n",
    "def set_index(data, col_name):\n",
    "    target_col = target_col(col_name)\n",
    "    data[col_name] = pd.to_datetime(data[col_name])\n",
    "    data.set_index(col_name,inplace=True)\n",
    "    #print(data.head())\n",
    "    return data\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225d60c",
   "metadata": {},
   "source": [
    "# Shape of Data / Rows & Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying rows and columns #Return dictionary .eg:{\"rows\": 0, \"cols\": 0}\n",
    "def shape_df(data):\n",
    "    s=data.shape\n",
    "    s1=s[0]\n",
    "    s2=s[1]\n",
    "    df={'Name':['row_count','col_count'],'Count':[s1,s2]}\n",
    "    return pd.DataFrame(df)\n",
    "    #use return here to print dict\n",
    "    #print('No of rows :{}'.format(s[0]))\n",
    "    #print('No of Columns:{}'.format(s[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86335a6e",
   "metadata": {},
   "source": [
    "# Head & Tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b14cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving choice to user to display head or tail  \n",
    "def display_head_tail(data, choice='Head'):\n",
    "    if choice == 'Head':\n",
    "        return data.head()\n",
    "    elif choice=='Tail':\n",
    "        return data.tail()\n",
    "    else:\n",
    "        return {\"message\": \"Invalid choice\"} #convert this into dataframe and display\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998034f",
   "metadata": {},
   "source": [
    "# Describe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display descriptive statistics #Columns rename as 'column_name'\n",
    "def describe_data(df):\n",
    "    return df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37354b95",
   "metadata": {},
   "source": [
    "# Resampling  Countinous/Discontinous (page 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e408cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= df.asfreq(pd.infer_freq(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd351be1",
   "metadata": {},
   "source": [
    "# Team is working on this will update you \n",
    "def check_Continuity(data):\n",
    "    c=pd.infer_freq(data.index)\n",
    "    if c==None:\n",
    "        print(\"This is non-continuous data\")\n",
    "        #Function for Resampling\n",
    "    else:\n",
    "        print(\"This is continuous data \")\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be443d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_Continuity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb51400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840fc54a",
   "metadata": {},
   "source": [
    "# Null Value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0f9d6",
   "metadata": {},
   "source": [
    "## List of columns having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This functions creates a dictionary where columns are keys and values are percentage of null values present in that column \n",
    "def null_list(df):\n",
    "    \n",
    "    mydict={}#an empty dictionary for storing null value percentage\n",
    "    list1=[]\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().sum()>0: #this is to create a dictionary with columns which has null values.\n",
    "            mydict[i]=[(df.isnull().sum())*100 / len(df)][0][i]\n",
    "    for j,k in mydict.items():\n",
    "        list1.append(j)\n",
    "    \n",
    "    if len(list1)==0: \n",
    "        print(\"This dataset doesn't have any null values , kindly proceed with the EDA .\\n\") #this print statment in terms of dict or dataframe\n",
    "    else:\n",
    "        return mydict# the output is in dictionary form "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4383ff1",
   "metadata": {},
   "source": [
    "## prefer dataframe first then Dictionary then at last json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109cc52",
   "metadata": {},
   "source": [
    "## Graph to display percentage of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting the null values. this function plots graph of columns in the x-axis and its percentage of null values in the y-axis\n",
    "def graph(data):\n",
    "\n",
    "    null_percentage = pd.DataFrame(data.isnull().sum()*100)/len(data)\n",
    "    # x=[data.columns]#convert this into list\n",
    "    # y=[null_percentage]# convert this into list\n",
    "    x = (np.array(data.columns)).tolist()\n",
    "    # print(x)\n",
    "    y = (np.array(null_percentage)).tolist()\n",
    "    # print(y)\n",
    "    print(null_percentage)\n",
    "\n",
    "    my_dict = {\n",
    "        \"x_label\": 'Columns',\n",
    "        \"y_label\": 'Pecentages',\n",
    "        \"title\": \"Percentage of null values present in each column\",\n",
    "        \"x_value\": x,\n",
    "        \"y_value\": y,\n",
    "        \"chart_type\": 'bar'\n",
    "    }\n",
    "\n",
    "    return my_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25157e",
   "metadata": {},
   "source": [
    "## Null Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe as an input and returns a dictionary with column names and null %\n",
    "def get_null_percentages(df):\n",
    "    mydict={}\n",
    "\n",
    "    for key in df.columns:\n",
    "        mydict[key] = [(df.isnull().sum())*100 / len(df)][0][key]\n",
    "    \n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dee029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe and column name as an input.\n",
    "def drop_rows(df, col_name):\n",
    "    return df.dropna(subset=[col_name], axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67120dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe and column name as an input.\n",
    "def drop_cols(df, col_name):\n",
    "    return df.drop([col_name],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes dataframe, column name, and impute method as an input.\n",
    "def impute(df, col_name, impute_method='interpolation'):\n",
    "\n",
    "    if df.dtypes[col_name] == str or df.dtypes[col_name] == object:\n",
    "        return df[col_name].fillna(df[col_name].mode()[0], inplace=True)\n",
    "\n",
    "    else:\n",
    "        flag1 = (df[col_name].isnull() & df[col_name].shift(-1).isnull()).any()\n",
    "        flag2 = df[col_name].head(1).isnull().bool()\n",
    "        flag3 = df[col_name].tail(1).isnull().bool()\n",
    "\n",
    "        if flag1 or flag2 or flag3:\n",
    "            return df[col_name].fillna(df[col_name].interpolate(method='linear', limit_direction=\"both\"), inplace=True)\n",
    "\n",
    "        elif impute_method == \"locf\" and (flag1 == False and flag2 == False and flag3 == False):\n",
    "            return df[col_name].fillna(df[col_name].ffill(), inplace=True)\n",
    "\n",
    "        elif impute_method == \"nocb\" and (flag1 == False and flag2 == False and flag3 == False):\n",
    "            return df[col_name].fillna(df[col_name].bfill(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e8595",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c283b58",
   "metadata": {},
   "source": [
    "## Date vs target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c50f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full plot of target column\n",
    "def plot_col(df,col_name):\n",
    "    title = '{}'.format(col_name)\n",
    "    df[target_col].plot(figsize=(12,6),title=title).autoscale(axis='both',tight=True)\n",
    "    print(\"Interpretation:\\n This graph represents visualization of dependent or target variable w.r.t Time.This depicts how the dependent variable varies with the time. X axis represents time and Y axis represents dependent variable. \")\n",
    "    #df.plot(ts_col,col_name,figsize=(12,6),title=title).autoscale(axis='both',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4dc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_col(df,col_name):\n",
    "    print(\"Interpretation:\\n This graph represents visualization of dependent or target variable w.r.t Time.This depicts how the dependent variable varies with the time. X axis represents time and Y axis represents dependent variable. \")\n",
    "   \n",
    "    my_dict={\"x_label\": 'Time',\n",
    "             \"y_label\":'Target column values',\n",
    "             \"title\": \"Target variable w.r.t. time\",\n",
    "             \"x_value\":df.index,\n",
    "             \"y_value\":df[target_col],\n",
    "             \"chart_type\":'lineplot'}\n",
    "    return my_dict\n",
    "     #df.plot(ts_col,col_name,figsize=(12,6),title=title).autoscale(axis='both',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd463982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an interactive plot of above code using plotly\n",
    "def plotly_line(data,col_name):\n",
    "    fig= px.line(x=data.index,y=data[col_name])\n",
    "    fig.show()\n",
    "    print(\"Interpretation:\\n This graph represents visualization of dependent or target variable w.r.t Time.This depicts how the dependent variable varies with the time. X axis represents time and Y axis represents dependent variable. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ab4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to show the alias image to the user\n",
    "from IPython.display import Image\n",
    "Image(filename=\"C:\\\\Users\\\\DB4\\\\Downloads\\\\MicrosoftTeams-image.png\",width=1000,height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5e585",
   "metadata": {},
   "source": [
    "## Resampled plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07353c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampled plot as per aliases input by user\n",
    "def resample_plot(data,col_name,resample_alias=\"M\"):\n",
    "    title = 'Resampled {} graph'.format(col_name)\n",
    "    \n",
    "    #resample_alias = input(\"Please enter an offset alias: \")#remove input func\n",
    "    data[col_name].resample(resample_alias).max().plot.bar(figsize=(16,6), title=title);\n",
    "    print(\"Interpretation:\\n Resampling:\\n Conversion of frequency of time in time series data. \\nThis graph represents visualization of resampled dependent or target variable w.r.t Time.This depicts how the resampled dependent variable varies with the time. X axis represents resampled or extended time and Y axis represents dependent variable. Main use of this plot is to show how the data behaves with different frequencies. This deals with the missing dates also, so it helps to make data continuous. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91043474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is from UI perspective\n",
    "def resample_plot(data,col_name,resample_alias=\"M\"):\n",
    "    data[col_name].resample(resample_alias).max()# resample aliaces ask to vijay\n",
    "    my_dict={\n",
    "        \"title\":\"Resampled Graph Of Target Variable\",\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Target Column',\n",
    "        \"x_values\":# yet to give \n",
    "        \"y_values\":# yet to decide\n",
    "        \"Chart_type\":\"BarPlot\"}\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f002d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly function for resampled plot\n",
    "def plotly_bar_resample(data,col_name,resample_alias=\"M\"):\n",
    "    #resample_alias = input(\"Please enter an offset alias: \") #removing all input func\n",
    "    fig=px.bar(data[col_name].resample(resample_alias).max(),y=col_name,color=col_name,color_continuous_scale=px.colors.sequential.Aggrnyl_r)\n",
    "    fig.update_layout(title_text='Resampled {} graph'.format(col_name))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce986d",
   "metadata": {},
   "source": [
    "## Top n Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying top n values in dataframe\n",
    "def top_n_values(data,col_name,n=10):\n",
    "    #n = int(input(\"How many top values do you want to see?\\n\"))\n",
    "    print(\"Below are the top {0} values in the {1} column: \".format(n, col_name))\n",
    "    return pd.DataFrame(data[col_name].sort_values(ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ab81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot for visualizing top n values\n",
    "def plot_top_n(data,col_name):\n",
    "    n= len(top_n_data)\n",
    "    fig = px.bar(data, x=data.index, y=col_name, labels={'x':'{}'.format(col_name)},\n",
    "             color=col_name, color_continuous_scale=px.colors.sequential.Brwnyl)\n",
    "    fig.update_layout(title_text='Top {} {} graph'.format(n,col_name))\n",
    "    fig.show()\n",
    "    print(\"Interpretation:\\n This graph represents visualization of Top values of dependent or target variable w.r.t Time. X axis represents time and Y axis represents top values of dependent variable. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n(data,col_name):\n",
    "    print(\"Interpretation:\\n This graph represents visualization of Top values of dependent or target variable w.r.t Time. X axis represents time and Y axis represents top values of dependent variable. \")\n",
    "    my_dict={\n",
    "        \"title\":\"Visualization of Top N values Of Target Variable\",\n",
    "        \"x_label\":'Date',\n",
    "        \"y_label\": 'Target Column',\n",
    "        \"x_values\": data.index,\n",
    "        \"y_values\":top_n_dataf.values,\n",
    "        \"Chart_type\":\"BarPlot\"}\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b231f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n(top_n_dataf,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n(top_n_dataf,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7972f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f861a7c8",
   "metadata": {},
   "source": [
    "# Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43d2c7",
   "metadata": {},
   "source": [
    "## Seasonal Decompose during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal decomposition plot\n",
    "def decomposition(series,choice):\n",
    "    \n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    from dateutil.parser import parse\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "   \n",
    "    \n",
    "\n",
    "    plt.rcParams.update({'figure.figsize': (20,10)})\n",
    "    y = series.to_frame()\n",
    "    \n",
    "    #choice=input(\"Enter 'M' for Multiplicative decomposition & 'A' for Additive decomposition & 'MA' for both :\\n\")\n",
    "    \n",
    "    if choice == 'M':\n",
    "        \n",
    "    # Multiplicative Decomposition \n",
    "       seasonal_decompose(y, model='multiplicative',period = 52).plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "    elif choice == 'A':\n",
    "        \n",
    "    # Additive Decomposition\n",
    "       seasonal_decompose(y, model='additive',period = 52).plot().suptitle('Additive Decompose', fontsize=22);\n",
    "    elif choice==\"MA\":\n",
    "        seasonal_decompose(y, model='multiplicative',period = 52).plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "        \n",
    "        seasonal_decompose(y, model='additive',period = 52).plot().suptitle('Additive Decompose', fontsize=22)\n",
    "    else:\n",
    "        print(\" This is invalid choice. Please choose Either M or A\")\n",
    "    print(\"\"\"Interpretation:\\n Here X axis represents Time and Y axis represents Normal scaled data. Time series has 4 components Trend,seasonality,cyclical variation and irregular variation. \\n Trend component: This is useful in predicting future movements. Over a long period of time, the trend shows whether the data tends to increase or decrease. \\n \n",
    "            Seasonal component: The seasonal component of a time series is the variation in some variable due to some predetermined patterns in its behavior. \\n Cyclical component: The cyclical component in a time series is the part of the movement in the variable which can be explained by other cyclical movements in the economy. \\n  irregular component: this term gives information about non-seasonal patterns.\\n\n",
    "            \\nTime series has two types of decomposition models Additive Model and Multiplicative model. The plot shows the decomposition of your time series data in its seasonal component, its trend component and the remainder. If you add or multiply the decomposition together you would get back the actual data. First block represents original series , second represents trend , third represents seasonality presents, fourth represents error component or residual. \n",
    "            \\nFor additive if we add below three blocks we get original data series. Similarly for multiplicative we have to multiply the components. \"\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition(df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b486b",
   "metadata": {},
   "source": [
    "## Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755af122",
   "metadata": {},
   "source": [
    "## Stationarity Check Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for checking stationarity\n",
    "def stationarity_check_plot(timeseries,col_name):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation for {}'.format(col_name))\n",
    "    plt.show(block=False)\n",
    "    print(\"Interpretation:\\n\\n Stationarity:\\n\\n Stationarity means that the statistical properties of a process generating a time series do not change over time. That is Mean and Standard deviation is approximately constant over time.\\n\\nStationarity Graph represents stationarity of the series w.r.t. Time. X axis depicts time and Y axis depicts Dependent variable . Blue line represents the original Time series data , Red line represents Mean of the series data and Black line represents standard deviation of the series. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check_plot(df,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adf test for checking stationarity and display output to user\n",
    "def adf_test(series):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "   \n",
    "    print(f'Augmented Dickey-Fuller Test: ')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string(), '\\n')          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")\n",
    "        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(df[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPSS test for stationarity and display output\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "def kpss_test(series):  \n",
    "    statistic, p_value, n_lags, critical_values = kpss(series)\n",
    "    # Format Output\n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critial Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38421d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of non stationarity to stationarity\n",
    "def non_stationarity_stationarity(data,series,adf,kpss):\n",
    "    #adf=input(\"Enter result of adf test either stationary or non stationary :\\n\")\n",
    "    #kpss=input(\"Enter result of kpss test either stationary or non stationary :\\n\")\n",
    "    \n",
    "    if adf==\"stationary\" and kpss==\"stationary\":\n",
    "        \n",
    "        print(\"Data has no unit root and is Stationary\")\n",
    "        \n",
    "    elif adf==\"non stationary\" and kpss==\"non stationary\":\n",
    "        \n",
    "        print(\"Data has unit root and is non stationary, please make data stationary\")\n",
    "        \n",
    "        choice=input(\"Enter T for Transformation method or D for differencing method :\\n\")\n",
    "    \n",
    "        if choice == 'T':\n",
    "            data['data_log']=np.sqrt(series)\n",
    "            data['data_diff']=data['data_log'].diff().dropna()\n",
    "            adf_test(data['data_diff']); \n",
    "        \n",
    "        elif choice == 'D':\n",
    "            data[\"diff_1\"] =series.diff(periods=1)\n",
    "            data['diff_1'].dropna()\n",
    "            adf_test(data['diff_1']); \n",
    "        \n",
    "        else:\n",
    "            print(\" This is invalid choice. Please choose Either T or D\")\n",
    "        \n",
    "    elif adf==\"non stationary\" and kpss==\"stationary\":\n",
    "        data['data_log']=np.sqrt(series)\n",
    "        data['data_diff']=data['data_log'].diff().dropna()\n",
    "        adf_test(data['data_diff']); \n",
    "        \n",
    "    elif adf==\"stationary\" and kpss==\"non stationary\":\n",
    "        \n",
    "        data[\"diff_1\"] =series.diff(periods=1)\n",
    "        data['diff_1'].dropna()\n",
    "        adf_test(data['diff_1']); \n",
    "        \n",
    "    else:\n",
    "        print(\"Please enter valid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stationarity_stationarity(df,df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72814a08",
   "metadata": {},
   "source": [
    "# ACF PACF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for ACF and PACF #ask priti about ideal number to lags to be considered as default value\n",
    "def acf_pacf(series,choice=15):\n",
    "\n",
    "\n",
    "\n",
    "    #choice=input(\"Ideal Choice for lags are considered to be 10% to 30% of the length of the data.That means lags between 10 to 30 might be used, Please choose Accordingly.:\\n\")\n",
    "    lags=int(choice)\n",
    "    plt.rcParams.update({'figure.figsize': (20,6)})\n",
    "\n",
    "    sm.graphics.tsa.plot_acf(series, lags=lags,title='auto correlation ',zero=False);\n",
    "    sm.graphics.tsa.plot_pacf(series, lags=lags,title='partial auto correlation ',zero=False);\n",
    "    print(\"Interpretation : \\n \")\n",
    "    print(\"\"\"ACF represnts auto correlation between varibles w.r.t Time into consideration all components of time series.PACF represnts correlation function of the variables with residuals partially . \\n\"\"\")\n",
    "    print(\"Both ACF & PACF starts at lag 0 , which is the correlation of variables with itself and therefore results in a correlation of 1. Difference between both is inclusion and exclusion of indirect correlations. Blue area depicts 95% confidence interval.\\n\")\n",
    "    print(\"CONCLUSION:\\n\")\n",
    "    print( \"\"\" Sharp Drop Point: \n",
    "            Instant drop lag just after lag 0.\n",
    "\n",
    "            ACF sharp drop point implies MA order & PACF sharp drop point implies AR order \n",
    "\n",
    "            Some basic approach for model choosing are as follows:\n",
    "\n",
    "            1. ACF plot declines gradually and PACF drops instantly use AR model.\n",
    "            2. ACF drops instantly and PACF declines gradually use MA model. \n",
    "            3. Both declines gradually use ARMA model\n",
    "            4. Both drops instantly we are not able to model the time series.\n",
    "\n",
    "            Note:\n",
    "\n",
    "            ARIMA and SARIMA models are Intergrated ARMA models we will use the same identified orders from both the plots.\n",
    "\n",
    "\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACF_PACF(df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1962c",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting dataset\n",
    "def split(data):\n",
    "    size_input=float(input(\"Please enter the size of percentage where you want to split the data-for eg 0.75 for 75% or 0.80 for 80%\"))\n",
    "    #splitting 85%/15% because of little amount of data\n",
    "    size = int(len(data) * size_input)\n",
    "    train= data[:size]\n",
    "    test = data[size:]\n",
    "    return(train,test)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test= split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76607aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4fcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb124dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e8eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701d372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec80547a",
   "metadata": {},
   "source": [
    "# Forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ccaa0",
   "metadata": {},
   "source": [
    "## AutoArima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40221429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoarima model\n",
    "def gen_auto_arima(df, col, m, f, periods, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5):\n",
    "    automodel= auto_arima(df[col], seasonal=True, m=m, start_p=0, start_q=0, d=None, D=None, stepwise=True, max_p= maxp, max_d= maxd, max_q = maxq,\n",
    "                         max_P= maxP, max_D= maxD, max_Q= maxQ)\n",
    "    print(automodel.summary())\n",
    "    preds, confint = automodel.predict(n_periods=periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1], periods = periods, freq=f)\n",
    "    fitted_series = pd.Series(preds, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "    print(preds)\n",
    "    plt.plot(df[target_col])\n",
    "    plt.plot(fitted_series, color='darkgreen')\n",
    "    fitted_series.to_excel('Output_forecast.xlsx')\n",
    "    plt.fill_between(lower_series.index,\n",
    "                 lower_series,\n",
    "                 upper_series,\n",
    "                 color='k', alpha=.15)\n",
    "    plt.savefig('Forecast_autoARIMA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_autoArima(train, target_col, 1, 'M', 98, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_auto_arima_plotly(df, col, m, f, periods, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5):\n",
    "    automodel= auto_arima(df[col], seasonal=True, m=m, start_p=0, start_q=0, d=None, D=None, stepwise=True, max_p= maxp, max_d= maxd, max_q = maxq,\n",
    "                         max_P= maxP, max_D= maxD, max_Q= maxQ)\n",
    "    print(automodel.summary())\n",
    "    preds, confint = automodel.predict(n_periods=periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1], periods = periods, freq=f)\n",
    "    fitted_series = pd.Series(preds, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "    print(preds)\n",
    "    fitted_series.to_excel('Output_forecast_plotly.xlsx')\n",
    "    fitted_dataframe=pd.DataFrame(fitted_series,index=index_of_fc)\n",
    "    pd.concat([train[target_col],fitted_dataframe],axis=1).iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985d023",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen_autoArima_plotly(train, target_col, 1, 'M', 49, maxp=5, maxd=2, maxq=5, maxP=5, maxD=2, maxQ=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a57860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_method(test_df):\n",
    "    y_hat_naive = test_df.copy()\n",
    "    y_hat_naive['naive_forecast'] = train[target_col][train_len-1]\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Naive Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_naive['naive_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_naive['naive_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_method(test_df):\n",
    "    y_hat_average = test_df.copy()\n",
    "    y_hat_average['average_forecast'] = train[target_col].mean()\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_average['average_forecast'], label='Average forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Average Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_average['average_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_average['average_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Average method'], 'MAPE': [mape], 'RMSE': [rmse]})\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45226ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_moving_average(df, ma_window):\n",
    "    y_hat_sma = df.copy()\n",
    "    y_hat_sma['sma_forecast'] = data[target_col].rolling(ma_window).mean()\n",
    "    y_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_sma['sma_forecast'], label='Simple moving average forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Simple Moving Average Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_sma['sma_forecast'][train_len:])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_sma['sma_forecast'][train_len:])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    results = results[['Method', 'RMSE', 'MAPE']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_exponential_smoothing(test_df,forecast_duration):\n",
    "    model = SimpleExpSmoothing(train[target_col])\n",
    "    model_fit = model.fit(smoothing_level=0.2,optimized=False)\n",
    "    model_fit.params\n",
    "    y_hat_ses = test_df.copy()\n",
    "    y_hat_ses['ses_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_ses['ses_forecast'], label='Simple exponential smoothing forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Simple Exponential Smoothing Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_ses['ses_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_ses['ses_forecast'])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holt_exponential_smoothing(test_df,seasonal_periods,forecast_duration):\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='additive', seasonal=None)\n",
    "    model_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\n",
    "    print(model_fit.params)\n",
    "    y_hat_holt = test_df.copy()\n",
    "    y_hat_holt['holt_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot( train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Holt\\'s Exponential Smoothing Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_holt['holt_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_holt['holt_forecast'])/test[target_col])*100,2)\n",
    "\n",
    "    results = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd940e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holtwinter_exponential_smoothing_additive(test_df,seasonal_periods,forecast_duration):    \n",
    "    y_hat_hwa = test.copy()\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='add', seasonal='add')\n",
    "    model_fit = model.fit(optimized=True)\n",
    "    y_hat_hwa['hw_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot( train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Holt Winters\\' Additive Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_hwa['hw_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_hwa['hw_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holtwinter_exponential_smoothing_multiplicative(test_df,seasonal_periods,forecast_duration):    \n",
    "    y_hat_hwa = test.copy()\n",
    "    model = ExponentialSmoothing(np.asarray(train[target_col]) ,seasonal_periods=seasonal_periods ,trend='add', seasonal='mul')\n",
    "    model_fit = model.fit(optimized=True)\n",
    "    y_hat_hwa['hw_forecast'] = model_fit.forecast(forecast_duration)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot( train[target_col], label='Train')\n",
    "    plt.plot(test[target_col], label='Test')\n",
    "    plt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s multiplicative forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Holt Winters\\' multiplicative Method')\n",
    "    plt.show()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_col], y_hat_hwa['hw_forecast'])).round(2)\n",
    "    mape = np.round(np.mean(np.abs(test[target_col]-y_hat_hwa['hw_forecast'])/test[target_col])*100,2)\n",
    "    results = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060cdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e5143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe172d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da478f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ed176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318ff85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47f2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a072bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956ef0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdfdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914238bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb283f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249d6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd71a71e5ba1ea9877d40fdb335f9bc992c084b8a69f1b667efd519ca9bd876f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
